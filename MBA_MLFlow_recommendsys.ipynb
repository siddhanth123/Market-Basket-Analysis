{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"3b39531d-0a8c-4912-b1d4-9aa1e60b6dca","showTitle":false,"title":""}},"source":["## Installation of PySpark and MLflow\n","\n","This code cell installs the necessary packages, PySpark and MLflow, using the `%pip` magic command in Jupyter Notebook.\n","\n","The first command, `%pip install pyspark`, installs the PySpark package. PySpark is the Python library for Apache Spark, a fast and general-purpose cluster computing system. It provides high-level APIs for distributed data processing, machine learning, and graph processing.\n","\n","The second command, `%pip install mlflow`, installs the MLflow package. MLflow is an open-source platform for managing the machine learning lifecycle. It offers tracking and versioning of experiments, packaging of models, and deployment capabilities.\n","\n","By executing these commands, you ensure that both PySpark and MLflow are installed in the environment, allowing you to use their functionalities for data processing and machine learning tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"32c2de33-f28a-452c-9f3b-19500060cc39","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Python interpreter will be restarted.\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","Collecting py4j==0.10.9.7\n","  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py): started\n","  Building wheel for pyspark (setup.py): finished with status 'done'\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317143 sha256=9ce1244fdab02d217089622f734f0fe05321fb1d0809bf055008b2a16e313bfc\n","  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.7 pyspark-3.4.0\n","Python interpreter will be restarted.\n","Python interpreter will be restarted.\n","Collecting mlflow\n","  Downloading mlflow-2.4.1-py3-none-any.whl (18.1 MB)\n","Collecting gunicorn<21\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","Collecting docker<7,>=4.0.0\n","  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n","Requirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\n","Collecting pyyaml<7,>=5.1\n","  Downloading PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\n","Collecting gitpython<4,>=2.1.0\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","Collecting cloudpickle<3\n","  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\n","Requirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\n","Requirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\n","Collecting alembic!=1.10.0,<2\n","  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\n","Requirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\n","Collecting Flask<3\n","  Downloading Flask-2.3.2-py3-none-any.whl (96 kB)\n","Requirement already satisfied: pyarrow<13,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\n","Requirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\n","Requirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\n","Requirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\n","Collecting markdown<4,>=3.3\n","  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n","Requirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\n","Collecting querystring-parser<2\n","  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n","Collecting importlib-metadata!=4.7.0,<7,>=3.7.0\n","  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n","Collecting sqlalchemy<3,>=1.4.0\n","  Downloading SQLAlchemy-2.0.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","Requirement already satisfied: packaging<24 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\n","Collecting sqlparse<1,>=0.4.0\n","  Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n","Collecting databricks-cli<1,>=0.8.7\n","  Downloading databricks-cli-0.17.7.tar.gz (83 kB)\n","Requirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\n","Collecting Mako\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","Requirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\n","Collecting pyjwt>=1.7.0\n","  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n","Collecting oauthlib>=3.1.0\n","  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n","Collecting tabulate>=0.7.7\n","  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n","Requirement already satisfied: six>=1.10.0 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.16.0)\n","Requirement already satisfied: urllib3<2.0.0,>=1.26.7 in /databricks/python3/lib/python3.9/site-packages (from databricks-cli<1,>=0.8.7->mlflow) (1.26.9)\n","Collecting websocket-client>=0.32.0\n","  Downloading websocket_client-1.5.3-py3-none-any.whl (56 kB)\n","Collecting Werkzeug>=2.3.3\n","  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n","Collecting click<9,>=7.0\n","  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n","Collecting itsdangerous>=2.1.2\n","  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n","Collecting blinker>=1.6.2\n","  Downloading blinker-1.6.2-py3-none-any.whl (13 kB)\n","Collecting Jinja2<4,>=2.11\n","  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.9/site-packages (from gunicorn<21->mlflow) (61.2.0)\n","Collecting zipp>=0.5\n","  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\n","Requirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\n","Requirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\n","Requirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\n","Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\n","Requirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\n","Collecting typing-extensions>=4\n","  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n","Collecting greenlet!=0.4.17\n","  Downloading greenlet-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (610 kB)\n","Collecting MarkupSafe>=2.0\n","  Downloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Building wheels for collected packages: databricks-cli\n","  Building wheel for databricks-cli (setup.py): started\n","  Building wheel for databricks-cli (setup.py): finished with status 'done'\n","  Created wheel for databricks-cli: filename=databricks_cli-0.17.7-py3-none-any.whl size=143878 sha256=816cd5e9b0a539542955e8050e13f285fbfed3ec77b25ed755e8f57adc34157e\n","  Stored in directory: /root/.cache/pip/wheels/b6/90/68/94d223a35a3910c1512a8d42d9f8333ce567ef26e250a56227\n","Successfully built databricks-cli\n","Installing collected packages: zipp, typing-extensions, smmap, MarkupSafe, greenlet, Werkzeug, websocket-client, tabulate, sqlalchemy, pyjwt, oauthlib, Mako, Jinja2, itsdangerous, importlib-metadata, gitdb, click, blinker, sqlparse, querystring-parser, pyyaml, markdown, gunicorn, gitpython, Flask, docker, databricks-cli, cloudpickle, alembic, mlflow\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 4.1.1\n","    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n","    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.0.1\n","    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n","    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n","  Attempting uninstall: Jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n","    Can't uninstall 'Jinja2'. No files were found to uninstall.\n","  Attempting uninstall: click\n","    Found existing installation: click 8.0.4\n","    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e1e459b5-6777-49bc-bd0c-acf76045ed73\n","    Can't uninstall 'click'. No files were found to uninstall.\n","Successfully installed Flask-2.3.2 Jinja2-3.1.2 Mako-1.2.4 MarkupSafe-2.1.3 Werkzeug-2.3.6 alembic-1.11.1 blinker-1.6.2 click-8.1.3 cloudpickle-2.2.1 databricks-cli-0.17.7 docker-6.1.3 gitdb-4.0.10 gitpython-3.1.31 greenlet-2.0.2 gunicorn-20.1.0 importlib-metadata-6.6.0 itsdangerous-2.1.2 markdown-3.4.3 mlflow-2.4.1 oauthlib-3.2.2 pyjwt-2.7.0 pyyaml-6.0 querystring-parser-1.2.4 smmap-5.0.0 sqlalchemy-2.0.16 sqlparse-0.4.4 tabulate-0.9.0 typing-extensions-4.6.3 websocket-client-1.5.3 zipp-3.15.0\n","Python interpreter will be restarted.\n"]}],"source":["%pip install pyspark\n","%pip install mlflow"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"beb661ac-7086-4b60-b0db-57a8db80b332","showTitle":false,"title":""}},"source":["## Importing Required Libraries\n","\n","In this code cell, we import several libraries that are necessary for the subsequent code execution. \n","\n","- `pyspark` is imported from `SparkContext`, `functions`, `SparkSession`, and `Column` modules. These modules provide essential functionalities for working with Spark and Spark SQL.\n","\n","- `FPGrowth` is imported from `pyspark.ml.fpm` module. It is a class that implements the FP-Growth algorithm for frequent pattern mining.\n","\n","- `mlflow` and `mlflow.spark` are imported to utilize the MLflow library. MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, and these imports enable MLflow integration with Spark.\n","\n","By importing these libraries, we ensure that the required functionality and classes are available for performing the subsequent data processing and modeling tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"b28845e8-ba5b-4570-a636-800aa50411bf","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark import SparkContext\n","from pyspark.sql import functions as f, SparkSession, Column\n","from pyspark.ml.fpm import FPGrowth\n","import mlflow\n","import mlflow.spark"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"94db1e81-cf81-46de-8a5e-848d0d32f6f3","showTitle":false,"title":""}},"source":["## Creating a Spark Session\n","\n","In this code cell, a Spark Session is created using the `SparkSession.builder` object. A Spark Session is the entry point for working with structured data in Spark and provides a programming interface to interact with various Spark functionalities.\n","\n","The `appName` parameter is set to \"MarketbasketMLFlow\", which specifies the name of the Spark application. This name helps identify the application in the Spark cluster.\n","\n","The `getOrCreate()` method is called on the `SparkSession.builder` object to either retrieve an existing Spark Session or create a new one if it doesn't exist. This ensures that only one Spark Session is created per application.\n","\n","By creating a Spark Session, we establish a connection to the Spark cluster and enable the execution of Spark operations on distributed data. The Spark Session provides a unified interface for working with structured data, including DataFrame and SQL operations, machine learning, and streaming capabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e801ef8e-a0de-4db9-b6d7-119a943c156d","showTitle":false,"title":""}},"outputs":[],"source":["spark = SparkSession.builder.appName(\"MarketbasketMLFlow\").getOrCreate()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"28ecde34-576b-470b-8c7c-fa878dcd11b7","showTitle":false,"title":""}},"source":["## Starting an MLflow Run\n","\n","In this code cell, an MLflow run is started using the `mlflow.start_run()` function. MLflow is an open-source platform for managing the end-to-end machine learning lifecycle, and it enables tracking of experiments, parameters, metrics, and artifacts.\n","\n","Within the MLflow run, the following steps are performed:\n","\n","1. Logging Spark Version: The Spark version is logged using the `mlflow.log_param()` function. This captures the version of Spark being used in the experiment.\n","\n","2. Reading the Data: Two CSV files, \"basket.csv\" and \"Groceries_data.csv\", are read into Spark DataFrames. The data is loaded using the `spark.read.csv()` function, and column names are assumed to be present in the first row (header=True). Additionally, a new column \"id\" is added to each DataFrame using the `f.monotonically_increasing_id()` function.\n","\n","3. Logging Data Paths: The paths of the input CSV files are logged using the `mlflow.log_param()` function. This provides a record of the data sources used in the experiment.\n","\n","4. Computing the Number of Baskets: The DataFrame `df_all` is grouped by the \"Member_number\" column, and the count of baskets for each member is computed. This information is stored in the `num_baskets` variable.\n","\n","5. Logging the Number of Baskets: The count of baskets is logged as a metric using the `mlflow.log_metric()` function. This metric represents the total number of baskets in the dataset.\n","\n","6. Removing Null Values: The DataFrame `df` is transformed to remove null values. The \"basket\" column is first selected, and then the null values within the array are removed using `f.array_except()` function.\n","\n","7. Performing Market Basket Analysis: The FP-Growth algorithm is applied to the aggregated DataFrame `df_aggregated` using the `FPGrowth` class from `pyspark.ml.fpm`. The minimum support and minimum confidence thresholds are set to 0.001. The resulting model is stored in the `model` variable.\n","\n","8. Logging the Model: The trained model is logged using the `mlflow.spark.log_model()` function. This saves the model artifacts to be later retrieved and used for predictions.\n","\n","9. Retrieving the Run ID: The run ID of the active MLflow run is obtained using `mlflow.active_run().info.run_id` and stored in the `run_id` variable. This run ID can be used to reference this specific MLflow run for later analysis or retrieval of logged information.\n","\n","The code cell performs various steps to log information, compute market basket analysis, and save the model using MLflow, allowing for better experimentation tracking and model management."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"cee48823-1133-4f74-82bd-bc1cedc3c5e5","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["2023/06/12 17:55:09 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n","/databricks/python/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n","  warnings.warn(\"Setuptools is replacing distutils.\")\n"]}],"source":["# Start an MLflow run\n","with mlflow.start_run():\n","    # Log the Spark version\n","    mlflow.log_param(\"spark_version\", spark.version)\n","\n","    # Read the data\n","    df = spark.read.csv(\"data/basket.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\n","    df_all = spark.read.csv(\"data/Groceries_data.csv\", header=True).withColumn(\"id\", f.monotonically_increasing_id())\n","\n","    # Log the data paths\n","    mlflow.log_param(\"basket_data_path\", \"data/basket.csv\")\n","    mlflow.log_param(\"groceries_data_path\", \"data/Groceries_data.csv\")\n","\n","    # Compute number of baskets\n","    num_baskets = df_all.groupBy(\"Member_number\").count()\n","\n","    # Log the number of baskets\n","    mlflow.log_metric(\"num_baskets\", num_baskets.count())\n","\n","    # Remove nulls\n","    df_basket = df.select(\"id\", f.array([df[c] for c in df.columns[:11]]).alias(\"basket\"))\n","    df_aggregated = df_basket.select(\"id\", f.array_except(\"basket\", f.array(f.lit(None))).alias(\"basket\"))\n","\n","    # Perform market basket analysis\n","    fp_growth = FPGrowth(minSupport=0.001, minConfidence=0.001, itemsCol='basket', predictionCol='prediction')\n","    model = fp_growth.fit(df_aggregated)\n","\n","    # Log the model\n","    mlflow.spark.log_model(model, \"market_basket_model\")\n","\n","    # Retrieve the run ID\n","    run_id = mlflow.active_run().info.run_id\n","    # print(\"MLflow run ID:\", run_id)"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"58dcb2b6-2250-407c-bf79-915b2d401921","showTitle":false,"title":""}},"source":["## Loading the Trained Model and Generating Predictions\n","\n","In this code cell, the trained model is loaded using MLflow and used to generate predictions on new data. The following steps are performed:\n","\n","1. Model Loading: The `model_uri` variable is set to the URI of the trained model artifact. The URI is constructed using the `run_id` obtained earlier in the code. This URI specifies the location of the model within MLflow. The `mlflow.spark.load_model()` function is then called to load the model into the `model` variable.\n","\n","2. Obtaining the FPGrowthModel: The FPGrowthModel from the loaded model is extracted. It is assumed that the FPGrowthModel is the last stage in the pipeline, so `model.stages[-1]` is used to access it. The FPGrowthModel is stored in the `fpgrowth_model` variable.\n","\n","3. Creating a PySpark DataFrame: A new PySpark DataFrame called `new_df` is created to hold the new data for which predictions will be generated. The DataFrame consists of a single column named 'basket', and it contains two rows of new data: (['beef'],) and (['oil'],). The `spark.sparkContext.parallelize()` function is used to parallelize the new data, and the resulting RDD is converted to a DataFrame using the `toDF()` function. The column names are specified using the `columns` list.\n","\n","4. Generating Predictions: The `model.transform()` function is used to generate predictions on the `new_df` DataFrame. The `model` variable contains the loaded model, and the `transform()` function applies the model to the input DataFrame. The predictions are stored in the `predictions` DataFrame.\n","\n","5. Showing the Recommendations: The `predictions.show(5)` statement is used to display the top 5 rows of the predictions DataFrame, which includes the generated recommendations based on the input data.\n","\n","By loading the trained model and generating predictions, this code cell demonstrates how to use the trained model to make recommendations for new data."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"11db1de6-d496-4c8a-9b8e-ab863e2ae466","showTitle":false,"title":""}},"outputs":[{"name":"stderr","output_type":"stream","text":["2023/06/12 17:56:07 INFO mlflow.spark: 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model' resolved as 'dbfs:/databricks/mlflow-tracking/1564150963134392/5af8033cf02a4c9aa777b9a67284c37e/artifacts/market_basket_model'\n","2023/06/12 17:56:11 INFO mlflow.spark: URI 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model/sparkml' does not point to the current DFS.\n","2023/06/12 17:56:11 INFO mlflow.spark: File 'runs:/5af8033cf02a4c9aa777b9a67284c37e/market_basket_model/sparkml' not found on DFS. Will attempt to upload the file.\n","2023/06/12 17:56:13 INFO mlflow.spark: Copied SparkML model to /tmp/mlflow/c8f5da9a-44c2-45d3-b873-801c0845a3c7\n"]},{"name":"stdout","output_type":"stream","text":["+------+--------------------+\n","|basket|          prediction|\n","+------+--------------------+\n","|[beef]|[frankfurter, rol...|\n","| [oil]|[rolls/buns, yogu...|\n","+------+--------------------+\n","\n"]}],"source":["# Load the trained model\n","model_uri = \"runs:/\"+run_id+\"/market_basket_model\"  # Replace <run-id> with the actual run ID of the model\n","model = mlflow.spark.load_model(model_uri)\n","\n","# Get the FPGrowthModel from the pipeline model\n","fpgrowth_model = model.stages[-1]  # Assuming the FPGrowthModel is the last stage in the pipeline\n","\n","# Create a PySpark DataFrame with new data\n","columns = ['basket']\n","new_data = [(['beef'],), (['oil'],)]\n","rdd = spark.sparkContext.parallelize(new_data)\n","new_df = rdd.toDF(columns)\n","\n","# Generate predictions using the model\n","predictions = model.transform(new_df)\n","\n","# Show the recommendations\n","predictions.show(5)\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"32d9203e-1511-4d82-a15f-9d87cd4b01d2","showTitle":false,"title":""}},"source":["## Evaluating Predictions using Lift\n","\n","This code cell demonstrates how to evaluate the predictions generated by the FPGrowth model using the concept of lift. The steps involved are as follows:\n","\n","1. Association Rules: The association rules learned by the FPGrowth model are stored in the `association_rules` variable. These rules represent the relationships between items in the dataset.\n","\n","2. Joining Predictions and Association Rules: The predictions generated earlier are joined with the association rules based on the common items. This is done using the `join()` function, where the join condition is specified as `association_rules.antecedent == predictions.basket`. The result is stored in the `eval_df` DataFrame.\n","\n","3. Calculating Lift: The lift is a measure of the strength of an association rule. In this code, the calculation of lift is commented out. The lift can be calculated by dividing the confidence of each association rule by the overall confidence of the rules. You can uncomment the relevant line of code (`eval_df = eval_df.withColumn(\"lift\", col(\"confidence\") / association_rules.select(\"confidence\").first())`) to calculate the lift for each association rule.\n","\n","4. Showing the Evaluation Results: Finally, the `eval_df.show()` statement is used to display the evaluation results. This will show the joined DataFrame with the association rules and the corresponding predictions. If you have uncommented the lift calculation, the lift values will also be included in the evaluation results.\n","\n","By evaluating the predictions using lift and examining the association rules, you can gain insights into the strength and significance of the relationships between different items in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"98180dc4-6bbf-4171-bde0-da6cadf2f7ca","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\n","|antecedent|          consequent|          confidence|               lift|             support|basket|          prediction|\n","+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\n","|    [beef]|              [curd]| 0.03740157480314961|  1.110396356705412|0.001269798837131...|[beef]|[frankfurter, rol...|\n","|    [beef]| [frozen vegetables]| 0.03740157480314961| 1.3356557608103283|0.001269798837131...|[beef]|[frankfurter, rol...|\n","|    [beef]|         [margarine]| 0.04133858267716536| 1.2832971215734963|0.001403461872619127|[beef]|[frankfurter, rol...|\n","|    [beef]|        [whole milk]|  0.1377952755905512| 0.8725479088706803|0.004678206242063757|[beef]|[frankfurter, rol...|\n","|    [beef]|[whipped/sour cream]| 0.04133858267716536| 0.9457939030556961|0.001403461872619127|[beef]|[frankfurter, rol...|\n","|    [beef]|    [tropical fruit]| 0.03346456692913386|0.49381687865939844|0.001136135801644...|[beef]|[frankfurter, rol...|\n","|    [beef]|            [butter]| 0.03346456692913386| 0.9501524003048006|0.001136135801644...|[beef]|[frankfurter, rol...|\n","|    [beef]|              [soda]|  0.0531496062992126| 0.5473348651446099|0.001804450979081735|[beef]|[frankfurter, rol...|\n","|    [beef]|     [shopping bags]| 0.03740157480314961| 0.7860109041847299|0.001269798837131...|[beef]|[frankfurter, rol...|\n","|    [beef]|      [citrus fruit]|  0.0531496062992126|  1.000349130886941|0.001804450979081735|[beef]|[frankfurter, rol...|\n","|    [beef]|       [brown bread]|0.045275590551181105| 1.2033013524286376|0.001537124908106...|[beef]|[frankfurter, rol...|\n","|    [beef]|  [other vegetables]| 0.08267716535433071| 0.6771201013666396|0.002806923745238254|[beef]|[frankfurter, rol...|\n","|    [beef]|            [pastry]| 0.03543307086614173| 0.6849935909174144|0.001202967319387...|[beef]|[frankfurter, rol...|\n","|    [beef]|     [bottled water]| 0.03937007874015748| 0.6487824759790488|0.001336630354875...|[beef]|[frankfurter, rol...|\n","|    [beef]|        [newspapers]| 0.04921259842519685| 1.2652373028113755|0.001670787943594199|[beef]|[frankfurter, rol...|\n","|    [beef]|[fruit/vegetable ...|0.031496062992125984| 0.9258852466624384|0.001069304283900...|[beef]|[frankfurter, rol...|\n","|    [beef]|            [yogurt]| 0.06496062992125984| 0.7564248291920708|0.002205440085544343|[beef]|[frankfurter, rol...|\n","|    [beef]|       [canned beer]| 0.02952755905511811| 0.6293744531933508|0.001002472766156...|[beef]|[frankfurter, rol...|\n","|    [beef]|      [bottled beer]|0.031496062992125984| 0.6950967412259308|0.001069304283900...|[beef]|[frankfurter, rol...|\n","|    [beef]|   [root vegetables]| 0.04921259842519685| 0.7073661001308554|0.001670787943594199|[beef]|[frankfurter, rol...|\n","+----------+--------------------+--------------------+-------------------+--------------------+------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Evaluate the predictions using lift\n","association_rules = fpgrowth_model.associationRules\n","\n","# Join the association rules with the predictions based on the common items\n","eval_df = association_rules.join(predictions, association_rules.antecedent == predictions.basket, \"inner\")\n","\n","# Calculate the lift for each association rule\n","# eval_df = eval_df.withColumn(\"lift\", col(\"confidence\") / association_rules.select(\"confidence\").first())\n","\n","# Show the evaluation results\n","eval_df.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"64fee9b1-ca56-4010-8d68-c5d46a7033b2","showTitle":false,"title":""}},"source":["## Generating Recommendations based on Association Rules\n","\n","In this code cell, we demonstrate how to generate recommendations based on the association rules learned by the FPGrowth model. Here are the steps involved:\n","\n","1. Specify the Input Item: You need to specify the item for which you want to generate recommendations. In the provided code, the variable `input_item` is set to \"beef\". You can replace it with your desired input item.\n","\n","2. Filtering the Association Rules: The association rules learned by the FPGrowth model are filtered based on the condition that the antecedent (left-hand side) of the rule contains the input item. This is done using the `fpgrowth_model.associationRules.filter()` function. The filtered rules are stored in the `recommendations` variable.\n","\n","3. Ordering the Recommendations: The filtered rules are then ordered by confidence, lift, and support in descending order using the `orderBy()` function. This ensures that the recommendations are ranked based on these criteria.\n","\n","4. Showing the Recommendations: Finally, the `recommendations.show()` statement is used to display the recommendations. This will show the association rules that contain the input item, ordered by confidence, lift, and support.\n","\n","By following these steps, you can generate recommendations based on the association rules and gain insights into the items that are likely to be associated with the input item."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"e9be490a-cc68-4c8e-a787-b3226730e586","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+--------------------+--------------------+-------------------+--------------------+\n","|antecedent|          consequent|          confidence|               lift|             support|\n","+----------+--------------------+--------------------+-------------------+--------------------+\n","|    [beef]|        [whole milk]|  0.1377952755905512| 0.8725479088706803|0.004678206242063757|\n","|    [beef]|  [other vegetables]| 0.08267716535433071| 0.6771201013666396|0.002806923745238254|\n","|    [beef]|            [yogurt]| 0.06496062992125984| 0.7564248291920708|0.002205440085544343|\n","|    [beef]|      [citrus fruit]|  0.0531496062992126|  1.000349130886941|0.001804450979081735|\n","|    [beef]|              [soda]|  0.0531496062992126| 0.5473348651446099|0.001804450979081735|\n","|    [beef]|        [newspapers]| 0.04921259842519685| 1.2652373028113755|0.001670787943594199|\n","|    [beef]|   [root vegetables]| 0.04921259842519685| 0.7073661001308554|0.001670787943594199|\n","|    [beef]|        [rolls/buns]|0.047244094488188976| 0.4294735029324251|0.001603956425850431|\n","|    [beef]|       [brown bread]|0.045275590551181105| 1.2033013524286376|0.001537124908106...|\n","|    [beef]|         [margarine]| 0.04133858267716536| 1.2832971215734963|0.001403461872619127|\n","|    [beef]|[whipped/sour cream]| 0.04133858267716536| 0.9457939030556961|0.001403461872619127|\n","|    [beef]|     [bottled water]| 0.03937007874015748| 0.6487824759790488|0.001336630354875...|\n","|    [beef]| [frozen vegetables]| 0.03740157480314961| 1.3356557608103283|0.001269798837131...|\n","|    [beef]|              [curd]| 0.03740157480314961|  1.110396356705412|0.001269798837131...|\n","|    [beef]|     [shopping bags]| 0.03740157480314961| 0.7860109041847299|0.001269798837131...|\n","|    [beef]|            [pastry]| 0.03543307086614173| 0.6849935909174144|0.001202967319387...|\n","|    [beef]|            [butter]| 0.03346456692913386| 0.9501524003048006|0.001136135801644...|\n","|    [beef]|     [domestic eggs]| 0.03346456692913386| 0.9022167837128469|0.001136135801644...|\n","|    [beef]|    [tropical fruit]| 0.03346456692913386|0.49381687865939844|0.001136135801644...|\n","|    [beef]|[fruit/vegetable ...|0.031496062992125984| 0.9258852466624384|0.001069304283900...|\n","+----------+--------------------+--------------------+-------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["import mlflow.pyfunc\n","from pyspark.sql import functions as F\n","\n","# Specify the input item for which you want to make recommendations\n","input_item = \"beef\"  # Replace with your desired input item\n","\n","# Generate recommendations using the association rules\n","recommendations = fpgrowth_model.associationRules.filter(F.array_contains(fpgrowth_model.associationRules.antecedent, input_item))\n","\n","# Order the filtered rules by confidence, lift, and support in descending order\n","recommendations = recommendations.orderBy(\n","    F.desc(\"confidence\"),\n","    F.desc(\"lift\"),\n","    F.desc(\"support\")\n",")\n","\n","\n","# Show the recommendations\n","recommendations.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{"byteLimit":2048000,"rowLimit":10000},"inputWidgets":{},"nuid":"4afea05c-6c00-47ba-998e-0f0da7626c64","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"The spark context has stopped and the driver is restarting. Your notebook will be automatically reattached.","errorTraceType":"html","metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["# Stop the Spark session\n","spark.stop()"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"Market_basket_recommedsys_MLFlow","widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
